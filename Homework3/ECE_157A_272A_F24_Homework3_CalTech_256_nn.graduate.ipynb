{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT0gnQuxKFTj"
   },
   "source": [
    "For this part of the homework, we will work with a subset of the CalTech-256 dataset, a widely recognized image dataset comprising a diverse collection of 256 object categories. This subset contains 10 specific classes from the CalTech-256 dataset. Your first task is to review the images in this dataset and compare how the classification task for this dataset differs from the other two datasets in this homework. You will then need to adjust and execute the code to observe how a Convolutional Neural Network (CNN) trained from scratch performs in comparison to a fine-tuned VGG16 model when applied to this specific dataset. Furthermore, you should evaluate and compare the performance of the two models on both the CalTech-256 dataset and the other dataset.\n",
    "\n",
    "For the coding task, your main objectives are as follows:\n",
    "\n",
    "+ Modify the data preprocessing functions, dataset class, and data loaders to be compatible with the CalTech-256 dataset. Please be aware that, unlike the HAM10000 dataset, the CalTech-256 dataset contains images of varying sizes.\n",
    "\n",
    "+ Modify the existing model architecture and the VGG16 model to work with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdPBom61n6Dl"
   },
   "source": [
    "# Download Dataset\n",
    "+ Download dataset from Box\n",
    "+ (You can download the images to your desktop by using the link https://ucsb.box.com/shared/static/vz9pmd1h7eexf2qxr5odvjy0xew5aqpo.zip)\n",
    "+ unzip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaTTgEgBnw6G"
   },
   "outputs": [],
   "source": [
    "!wget -nc -O datasets.zip https://ucsb.box.com/shared/static/vz9pmd1h7eexf2qxr5odvjy0xew5aqpo.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XqNuwYYoJql"
   },
   "outputs": [],
   "source": [
    "!unzip -n dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI4S6xMupDUs"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwAjofYzo5pa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Final, List, Tuple, NamedTuple, Type, Union\n",
    "from tqdm.auto import tqdm, trange\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN3MkZVhpOqy"
   },
   "source": [
    "# Type Definition\n",
    "Define some types for clearer labeling of our interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYDAX8YMpO06"
   },
   "outputs": [],
   "source": [
    "TODO = NotImplemented  # NOTE: This is a placeholder for code that needs to be written\n",
    "\n",
    "Loss: Type = float\n",
    "Accuracy: Type = float\n",
    "\n",
    "\n",
    "class LossAcc(NamedTuple):\n",
    "    loss: Loss\n",
    "    accuracy: Accuracy\n",
    "\n",
    "\n",
    "class TrainValLossAcc(NamedTuple):\n",
    "    train: List[List[LossAcc]]\n",
    "    val: List[LossAcc]\n",
    "\n",
    "\n",
    "example_result_pair: LossAcc = LossAcc(1.6, 0.7)\n",
    "print(\"loss:\", example_result_pair.loss)\n",
    "print(\"accuracy:\", example_result_pair.accuracy)\n",
    "print(example_result_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNLnm1SepbcK"
   },
   "source": [
    "# Configurations\n",
    "Some configuration options that are required to make the dataset work with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISWwbiWBOLBN"
   },
   "outputs": [],
   "source": [
    "num_epochs: Final[int] = 20\n",
    "\n",
    "batch_size: Final[int] = 16\n",
    "\n",
    "valid_set_size: Final[float] = 0.30\n",
    "\n",
    "print(\"Number of epochs: \", num_epochs)\n",
    "print(\"Batch size: \", batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY3rskMyp8lN"
   },
   "source": [
    "# Loading CalTech-256 training data from directory\n",
    "\n",
    "1. Define Data Transforms:\n",
    "\n",
    "  + Define data transformations using `transforms.Compose`. This should include\n",
    "    + Padding to make the image a square.\n",
    "    + Conversion to a tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range `[0, 255]` to a torch.FloatTensor of shape (C x H x W) in the range `[0.0, 1.0]`.\n",
    "    + Resizing to (224, 224).\n",
    "    + Optionally, you can include other preprocessing steps.\n",
    "  + Padding is added to maintain the aspect ratio of the image content when resizing to (224, 224).\n",
    "\n",
    "2. Load the CalTech-256 Dataset:\n",
    "\n",
    "  + Set the path to your CalTech-256 `train` dataset directory.\n",
    "  + Create the dataset using the `datasets.ImageFolder` class. This class automatically assigns labels based on the subdirectory names.\n",
    "\n",
    "3. Get Class Labels:\n",
    "\n",
    "  + Obtain the class labels (class names) from the `all_dataset.classes` attribute. This will give you a list of class labels in the order they appear in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlstPqA6XkYB"
   },
   "outputs": [],
   "source": [
    "# Define a custom data transform to pad the image to be square\n",
    "class SquarePad(object):\n",
    "    def __init__(self, fill_color=(0, 0, 0)):\n",
    "        self.fill_color = fill_color\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "\n",
    "        # Calculate the size of the square canvas\n",
    "        size = max(width, height)\n",
    "\n",
    "        # Create a new blank image with the square size\n",
    "        new_img = Image.new('RGB', (size, size), self.fill_color)\n",
    "\n",
    "        # Paste the original image onto the center of the square canvas\n",
    "        new_img.paste(img, ((size - width) // 2, (size - height) // 2))\n",
    "\n",
    "        return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzvff_sUPpQY"
   },
   "outputs": [],
   "source": [
    "# Define the data transformations composing of\n",
    "# - Padding to make the image a square\n",
    "# - Conversion to a tensor\n",
    "# - Resize to size (224, 224)\n",
    "# Compose: https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n",
    "data_transforms =# CODE HERE\n",
    "    # You can add more transformations as needed, e.g., normalization\n",
    "\n",
    "# Set the path to your CalTech-256 train dataset directory\n",
    "data_dir =# CODE HERE\n",
    "\n",
    "# Create a dataset using ImageFolder\n",
    "# ImageFolder: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "all_dataset =# CODE HERE\n",
    "\n",
    "# get the classes attribute from all_dataset\n",
    "classes: Final[List[str]] =# CODE HERE\n",
    "\n",
    "num_classes: Final[int] = len(classes)\n",
    "print(f'Number of classes: {len(classes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LORXsdND-yTX"
   },
   "source": [
    "PyTorch provides the `torch.utils.data.random_split()` function to split a dataset. This method takes a dataset and a list of lengths/ratios, and returns a list of datasets of the specified lengths/ratios. If lengths are given, the sum of the lengths must equal the length of the original dataset. If ratios are given, the sum of the ratios must equal to 1. In the configuration block above, we have already computed `test_set_sample_count` and `test_set_actual_size` for you, so you can use it to create a train/valid split.\n",
    "\n",
    "To get a consistent split, you are able to set your own random seed, though the process is more complex than for `scikit-learn`. To do so, add the argument `generator=torch.Generator().manual_seed(9)` to the `random_split()` function call. This split has produced a good class balance in our testing, but you are welcome to experiment with different `valid_set_size` and `manual_seed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G91VDMMTqgsv"
   },
   "outputs": [],
   "source": [
    "# random_split: https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
    "train_dataset, val_dataset =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ashnU7uDkTXw"
   },
   "source": [
    "Once arranged into a `Dataset`, we wrap the `Dataset` in a `Dataloader` object. This object is used to iterate over the dataset in batches, shuffle the dataset, and perform other useful operations, like parallel data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKrjx_AjkWob"
   },
   "outputs": [],
   "source": [
    "# DataLoader: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders\n",
    "train_dataloader =# CODE HERE\n",
    "val_dataloader =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EIeDOmqfKn"
   },
   "source": [
    "The below visualization of class label balances is provided for your reference. The train/valid split should produce similar class label balances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIeGo4yiTO7x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to plot class label distribution as ratios\n",
    "def plot_class_distribution_ratios(train_dataset, val_dataset):\n",
    "    class_counts_train = [0] * len(train_dataset.dataset.classes)\n",
    "    class_counts_val = [0] * len(val_dataset.dataset.classes)\n",
    "\n",
    "    for _, label in train_dataset:\n",
    "        class_counts_train[label] += 1\n",
    "\n",
    "    for _, label in val_dataset:\n",
    "        class_counts_val[label] += 1\n",
    "\n",
    "    # Calculate the total number of samples in the entire dataset\n",
    "    total_train_samples = sum(class_counts_train)\n",
    "    total_valid_samples = sum(class_counts_val)\n",
    "\n",
    "    # Calculate class ratios\n",
    "    class_ratios_train = [count / total_train_samples for count in class_counts_train]\n",
    "    class_ratios_val = [count / total_valid_samples for count in class_counts_val]\n",
    "\n",
    "    class_labels = train_dataset.dataset.classes\n",
    "    width = 0.35\n",
    "    x = np.arange(len(class_labels))\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.bar(x - width/2, class_ratios_train, width, label='Training Set')\n",
    "    plt.bar(x + width/2, class_ratios_val, width, label='Validation Set')\n",
    "    plt.xlabel('Class Label')\n",
    "    plt.ylabel('Class Ratio (to Total Samples)')\n",
    "    plt.title('Class Ratios in Training and Validation Sets')\n",
    "    plt.xticks(x, class_labels, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot class ratios for training and validation sets\n",
    "plot_class_distribution_ratios(train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krjk8Nstyfx8"
   },
   "source": [
    "# Custom Neural Network Model\n",
    "\n",
    "Now we'll define a new network. We'll use this network to train on the CalTech-256 dataset. We'll use the `torch.nn` module to define the network. This module provides a number of useful building blocks for neural networks, including linear layers, convolution layers, dropout layers, maxpooling layers, activation functions, flatten function, and loss functions.\n",
    "\n",
    "If this is your absolute first interaction with PyTorch, you may find [this basic tutorial link](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) and [this CNN tutorial link](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network) to be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAfDaho6yZK5"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CalTechNetwork(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(CalTechNetwork, self).__init__()\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 14 * 14, 128)  # Adjust the input size based on your needs\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)  # Apply dropout after the first convolutional layer\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)  # Apply dropout after the second convolutional layer\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)  # Apply dropout after the third convolutional layer\n",
    "        x = self.pool(nn.functional.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)  # Apply dropout after the fourth convolutional layer\n",
    "        x = x.view(x.size(0), -1)  # Adjust the input size based on your needs\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after the first fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CalTechNetwork model with dropout\n",
    "dropout_prob = 0.5  # You can adjust the dropout probability as needed\n",
    "model = CalTechNetwork(dropout_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBIEFjmxsRvh"
   },
   "source": [
    "# Validate Function\n",
    "\n",
    "We will define the `validate` function that evaluates the performance of the model after every training epoch. This function simply puts the network into `eval` mode, then iterates over the validation dataset and computes the accuracy and loss of the network.\n",
    "\n",
    "Accuracy computations you can perform in similar manner as you have in the past assignments. For loss computations, we will use the `torch.nn.functional.cross_entropy` function, which assumes the network outputs class logits, and computes the cross entropy loss between the logits and the ground truth labels.\n",
    "\n",
    "Some components of models behave differently whether they're under training or validation. For example, dropout layers will randomly drop out nodes during training, but will not do so during validation. To handle this, PyTorch provides the `.train()` method to set the model into training mode and the `.eval()` method to set the model into validation mode. We will use this to set the model into validation mode before computing the accuracy and loss.\n",
    "\n",
    "Running data through a model is as simple as treating the model as a function! That is, `model(data)` will run the data through the model and return the output. We can then use this output to compute the accuracy and loss.\n",
    "\n",
    "The result of `torch.tensor` computations is always more `torch.tensor`s, so use the `.item()` method to get the actual value of the loss and/or accuracy when you are ready to store it.\n",
    "\n",
    "To leverage a GPU for speeding up the computation, you should move the model, data, and labels to the GPU when a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFAChrXRsReg"
   },
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, val_data: DataLoader) -> LossAcc:\n",
    "    \"\"\"\n",
    "    Returns the loss and accuracy of the model on the validation data.\n",
    "    \"\"\"\n",
    "    # Check for GPU availability:\n",
    "    # This code assigns the GPU (\"cuda\") as the device if a GPU is available;\n",
    "    # otherwise, it defaults to the CPU.\n",
    "    # Hint: search for `torch.device` and `torch.cuda.is_available`\n",
    "    device =# CODE HERE\n",
    "\n",
    "    # Move the model to the GPU using the `.to` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # The below line sets up an environment that prevents\n",
    "    # the model from updating its weights during validation.\n",
    "    # This is helpful because we don't want to update the weights,\n",
    "    # so using this line saves us expensive gradient computations.\n",
    "    with torch.no_grad():\n",
    "        # Set up any tracking variables you'd like to use in order to compute\n",
    "        # the accuracy and average loss over all batches\n",
    "        # CODE HERE\n",
    "\n",
    "        # Iterate through the batches of validation data\n",
    "        # CODE HERE\n",
    "\n",
    "            # Get the data and labels from the batch\n",
    "            # CODE HERE\n",
    "\n",
    "            # Move the data and labels to the GPU using the `.to` method\n",
    "            # CODE HERE\n",
    "\n",
    "            # Get the model's predictions for the data\n",
    "            # CODE HERE\n",
    "\n",
    "            # Calculate the average loss of this batch\n",
    "            # using `nn.functional.cross_entropy`\n",
    "            # CODE HERE\n",
    "\n",
    "            # Calculate the number of correct predictions and\n",
    "            # retrieve the number of samples in this batch\n",
    "            # CODE HERE\n",
    "\n",
    "        # Compute the accuracy and average loss over all batches.\n",
    "        accuracy =# CODE HERE\n",
    "        average_loss =# CODE HERE\n",
    "\n",
    "        return LossAcc(loss=average_loss, accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22Hr1XAyu9Pq"
   },
   "source": [
    "# Training Functions\n",
    "Now, to train a model, we have to write training functions. To ease understanding, we've broken the writing of a \"training loop\" into three functions: `train_batch` to handle a single batch, `train_epoch` to handle a single epoch, and `train_loop` to handle training as many steps as you would like.\n",
    "\n",
    "Remember, we're still using `nn.functional.cross_entropy` to compute the loss. Don't forget to set the model to `.train()` mode before training!\n",
    "\n",
    "After setting the model to training, we need to zero out the gradients from the previous step. This is because PyTorch accumulates gradients from each step, and we don't want to accumulate multiple steps of gradients after we've already used their gradients to update the model. We can zero out the gradients by calling the `.zero_grad()` method on the optimizer. Once we've computed the loss, we can call the `.backward()` method on the loss to compute the gradients. Finally, we can call the `.step()` method on the optimizer to update the model weights.\n",
    "\n",
    "To leverage a GPU for speeding up the computation, you should move the model, data, and labels to the GPU when a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8Lzk56Nr5h7"
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    inputs: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    ") -> LossAcc:\n",
    "    \"\"\"\n",
    "    Trains the model on a single batch of data.\n",
    "    Returns the loss and accuracy of the model on the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model to training mode\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Zero the gradients\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Get the model's predictions for the data\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Calculate the loss using `nn.functional.cross_entropy`\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Backpropagate the loss using `backward` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Update the model's weights using `step` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Calculate the accuracy\n",
    "        # CODE HERE\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        # CODE HERE\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module, optimizer: optim.Optimizer, train_data: DataLoader\n",
    ") -> List[LossAcc]:\n",
    "    \"\"\"\n",
    "    Trains the model on a single epoch of data.\n",
    "    Returns a list of the loss and accuracy of the model on each batch.\n",
    "    \"\"\"\n",
    "    # Check for GPU availability:\n",
    "    # This code assigns the GPU (\"cuda\") as the device if a GPU is available;\n",
    "    # otherwise, it defaults to the CPU.\n",
    "    # Hint: search for `torch.device` and `torch.cuda.is_available`\n",
    "    device =# CODE HERE\n",
    "\n",
    "    # Move the model to the GPU using the `.to` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Set up a list to store the LossAcc (loss and accuracy) of the batches\n",
    "    # CODE HERE\n",
    "\n",
    "    # Iterate through the batches of training data\n",
    "    # CODE HERE\n",
    "\n",
    "        # Get the data and labels from the batch\n",
    "        # CODE HERE\n",
    "\n",
    "        # Move the data and labels to the GPU using the `.to` method\n",
    "        # CODE HERE\n",
    "\n",
    "        # Train the model on the batch with `train_batch`\n",
    "        # CODE HERE\n",
    "\n",
    "        # Add the LossAcc to the list\n",
    "        # CODE HERE\n",
    "\n",
    "    # Return the list of LossAcc\n",
    "    # CODE HERE\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    train_data: DataLoader,\n",
    "    val_data: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    ") -> TrainValLossAcc:\n",
    "    \"\"\"\n",
    "    Trains the model on the training data for `num_epochs` epochs.\n",
    "    Returns the loss and accuracy of the model on the training and validation data,\n",
    "    taken per-batch in training and per-epoch in validation.\n",
    "    \"\"\"\n",
    "    # Set up a list to store the lists of training List[LossAcc]\n",
    "    # and another list to store the validation LossAcc\n",
    "    # CODE HERE\n",
    "\n",
    "    # Initialize variables to track the best model's state dictionary and\n",
    "    # best validation loss\n",
    "    # CODE HERE\n",
    "\n",
    "    # Iterate through the epochs\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training progress...'):\n",
    "\n",
    "        # Train the model on the epoch with `train_epoch`\n",
    "        # CODE HERE\n",
    "\n",
    "        # Validate the model on the validation data\n",
    "        # CODE HERE\n",
    "\n",
    "        # Add the loss and accuracy to the lists\n",
    "        # CODE HERE\n",
    "\n",
    "        # Update best model's state dictionary and best validation loss\n",
    "        # if validation loss is lower than the best validation loss\n",
    "        # CODE HERE\n",
    "\n",
    "    # Load the model state dictionary with the best model's state dictionary\n",
    "    # CODE HERE\n",
    "\n",
    "    # Return the lists of loss and accuracy\n",
    "    # CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3s6Ws-OzmVu"
   },
   "source": [
    "# Train Custom Neural Network\n",
    "Finally, we can train your neural network! We'll use basic Stochastic Gradient Descent (SGD) as the optimization routine to train the network. We'll use a learning rate of 0.1, and train for 20 epochs. Feel free to change the optimizer and training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hT6Fr0a3zRAK"
   },
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "history: TrainValLossAcc =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_VHTU6J2mXb"
   },
   "source": [
    "We provide this visualization code, to give you a picture of what your model learned while it was training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-CtC6u5z5do"
   },
   "outputs": [],
   "source": [
    "training_history = pd.DataFrame.from_records(\n",
    "    [loss for batch_losses in history.train for loss in batch_losses],\n",
    "    columns=LossAcc._fields,\n",
    ")\n",
    "training_history[\"epoch\"] = training_history.index / len(train_dataloader)\n",
    "validation_history = pd.DataFrame.from_records(history.val, columns=LossAcc._fields)\n",
    "validation_history[\"epoch\"] = validation_history.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLrEIoRq2pvY"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(training_history[\"epoch\"], training_history[\"loss\"], label=\"Training\")\n",
    "ax[0].plot(validation_history[\"epoch\"], validation_history[\"loss\"], label=\"Validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(training_history[\"epoch\"], training_history[\"accuracy\"], label=\"Training\")\n",
    "ax[1].plot(\n",
    "    validation_history[\"epoch\"], validation_history[\"accuracy\"], label=\"Validation\"\n",
    ")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOYkT1yFyxTN"
   },
   "outputs": [],
   "source": [
    "# Get the validation accuracy of the best model\n",
    "# Get the accuracy corresponding to the lowest loss\n",
    "lowest_loss = validation_history['loss'].min()\n",
    "best_accuracy = validation_history.loc[validation_history['loss'] == lowest_loss, 'accuracy'].item()\n",
    "print(f'Best validation accuracy during training: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWkXoD37WX8"
   },
   "source": [
    "# Test Data Prediction: Custom Neural Network Model\n",
    "Make Predictions for the Test Data. This function is already defined for you, you just need to call it and pass in the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4spDiw7V4l"
   },
   "outputs": [],
   "source": [
    "def generate_test_predictions(model, data_transforms, test_data_dir, output_csv_path, classes):\n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_dataset = datasets.ImageFolder(test_data_dir, transform=data_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store filenames and predicted class labels\n",
    "    filenames = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate through the test dataset and make predictions\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Convert integer labels to class names\n",
    "            predicted_class_names = [classes[p] for p in predicted]\n",
    "\n",
    "            # predicted class labels\n",
    "            predicted_labels.extend(predicted_class_names)\n",
    "\n",
    "\n",
    "    # Get the basenames of the files\n",
    "    basenames = [os.path.basename(filename) for filename, _ in test_dataset.imgs]\n",
    "\n",
    "    # Create a DataFrame to store basenames and predicted labels\n",
    "    data = {'Basename': basenames, 'Predictions': predicted_labels}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Sort the DataFrame by Basename\n",
    "    df.sort_values(by='Basename', inplace=True)\n",
    "\n",
    "    # drop the Basename column\n",
    "    df.drop(columns=['Basename'], inplace=True)\n",
    "\n",
    "    # Save the sorted predictions to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Predictions saved to {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv8a_6ao8s-N"
   },
   "outputs": [],
   "source": [
    "# Generate test predictions and output the predictions to CSV named 'caltech256_custom_nn_prediction.csv'\n",
    "generate_test_predictions(\n",
    "    model =# CODE HERE\n",
    "    data_transforms =# CODE HERE\n",
    "    test_data_dir =# CODE HERE\n",
    "    output_csv_path =# CODE HERE\n",
    "    classes =# CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdBQp71t39Ty"
   },
   "source": [
    "# Fine-Tune VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQTLE4N44VQx"
   },
   "source": [
    "Now, we will be fine-tuning a VGG16 model to perform the same CalTech-256 classification task and compare the performance against the custom model which is trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vp370LKLkQJw"
   },
   "source": [
    "# Loading CalTech-256 training data from directory (VGG16)\n",
    "\n",
    "1. Define Data Transforms:\n",
    "\n",
    "  + Define data transformations using `transforms.Compose`. This should include\n",
    "    + Padding to make the image a square.\n",
    "    + Conversion to a tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range `[0, 255]` to a torch.FloatTensor of shape (C x H x W) in the range `[0.0, 1.0]`.\n",
    "    + Resizing to (224, 224).\n",
    "    + Normalization using the mean and standard deviation values `[0.485, 0.456, 0.406]` and `[0.229, 0.224, 0.225]`. These values are used for the pre-train ImageNet Dataset to set the values to be in the range `[-1, 1]`.\n",
    "    + Optionally, you can include other preprocessing steps.\n",
    "  + Padding is added to maintain the aspect ratio when resizing to (224, 224). The resizing to (224, 224) and normalization are performed to ensure that the images match the input shape of VGG16 in the later parts.\n",
    "\n",
    "2. Load the CalTech-256 Dataset:\n",
    "\n",
    "  + Set the path to your CalTech-256 `train` dataset directory.\n",
    "  + Create the dataset using the `datasets.ImageFolder` class. This class automatically assigns labels based on the subdirectory names.\n",
    "\n",
    "3. Get Class Labels:\n",
    "\n",
    "  + Obtain the class labels (class names) from the `all_dataset.classes` attribute. This will give you a list of class labels in the order they appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOsUdSTQk5Zb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define the data transformations composing of ToTensor and\n",
    "# - Padding to make the image a square\n",
    "# - Conversion to a tensor\n",
    "# - Resize to size (224, 224)\n",
    "# - Normalization using the mean and standard deviation values\n",
    "#  `[0.485, 0.456, 0.406]` and `[0.229, 0.224, 0.225]`\n",
    "# Compose: https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html\n",
    "data_transforms =# CODE HERE\n",
    "\n",
    "# Create a dataset using ImageFolder\n",
    "# ImageFolder: https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "all_dataset =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8vxpIwbmLrg"
   },
   "source": [
    "Split Dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDp6zB0dmMQ8"
   },
   "outputs": [],
   "source": [
    "# random_split: https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
    "train_dataset, val_dataset =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg9fKn8_mN5S"
   },
   "source": [
    "Once arranged into a `Dataset`, we wrap the `Dataset` in a `Dataloader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nGzPVUxmQeJ"
   },
   "outputs": [],
   "source": [
    "# DataLoader: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders\n",
    "train_dataloader =# CODE HERE\n",
    "val_dataloader =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbxtR7kREi0f"
   },
   "source": [
    "# Overview of Fine-Tuning VGG16\n",
    "\n",
    "VGG16 is a popular deep convolutional neural network architecture used for various computer vision tasks. It consists of two main components: the \"features\" part and the \"classifier\" part. Fine-tuning VGG16 involves adjusting these components to adapt the pre-trained model to a new task.\n",
    "\n",
    "Components of VGG16\n",
    "1. Features Layers:\n",
    "The \"features\" part of VGG16 comprises a stack of convolutional and max-pooling layers. These layers serve as feature extractors and are responsible for capturing hierarchical features from the input image.\n",
    "These layers are typically frozen during fine-tuning when using transfer learning. Freezing means that the weights of these layers are not updated during training to preserve the pre-trained knowledge.\n",
    "2. Classifier Layers:\n",
    "The \"classifier\" part of VGG16 consists of fully connected layers (also known as dense layers). These layers are responsible for making predictions based on the extracted features.\n",
    "During fine-tuning, the classifier layers are often replaced by a new set of fully connected layers to adapt the model to the specific task.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnbVLEPTExn-"
   },
   "source": [
    "Load the Pre-Trained VGG16 Model:\n",
    "\n",
    "Begin by loading the pre-trained VGG16 model with weights 'IMAGENET1K_V1', which includes both the features and classifier components. You can do this using PyTorch's torchvision.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EEZ8p7k2qsC"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# load VGG16 models with weights 'IMAGENET1K_V1'\n",
    "# CODE HERE\n",
    "model =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBNEPlAcD88p"
   },
   "source": [
    "Freeze the Features Layers:\n",
    "\n",
    "Disable gradient updates to prevent them from changing during training. This step ensures that the model retains its ability to extract relevant features from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGU_IYWq6He2"
   },
   "outputs": [],
   "source": [
    "# Get the features part of the model\n",
    "features =# CODE HERE\n",
    "\n",
    "# Freeze the feature layers\n",
    "# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmoBdEG5FXvr"
   },
   "source": [
    "Define and Replace the Classifier:\n",
    "\n",
    "\n",
    "+ Replace only the last layer to have 10 output units for the CalTech-256 dataset. Note: Last classifier layer has 4096 input units.\n",
    "+ Set the last layer to require gradients for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-1XLZEs5enP"
   },
   "outputs": [],
   "source": [
    "# Modify the last layer to have 10 output units for the CalTech-256 dataset\n",
    "# CODE HERE\n",
    "\n",
    "# Set the last layer to require gradients for fine-tuning\n",
    "# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSXrVI-V4Voq"
   },
   "source": [
    "# Fine-Tune VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyfsOfAO4Q3m"
   },
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "history: TrainValLossAcc =# CODE HERE\n",
""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O05P97Vcw7HB"
   },
   "source": [
    "We provide this visualization code, to give you a picture of what your model learned while it was training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vEQM-6K4S_W"
   },
   "outputs": [],
   "source": [
    "training_history = pd.DataFrame.from_records(\n",
    "    [loss for batch_losses in history.train for loss in batch_losses],\n",
    "    columns=LossAcc._fields,\n",
    ")\n",
    "training_history[\"epoch\"] = training_history.index / len(train_dataloader)\n",
    "validation_history = pd.DataFrame.from_records(history.val, columns=LossAcc._fields)\n",
    "validation_history[\"epoch\"] = validation_history.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20v5Zp0z4UbS"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(training_history[\"epoch\"], training_history[\"loss\"], label=\"Training\")\n",
    "ax[0].plot(validation_history[\"epoch\"], validation_history[\"loss\"], label=\"Validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(training_history[\"epoch\"], training_history[\"accuracy\"], label=\"Training\")\n",
    "ax[1].plot(\n",
    "    validation_history[\"epoch\"], validation_history[\"accuracy\"], label=\"Validation\"\n",
    ")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiSZyP9QxukH"
   },
   "outputs": [],
   "source": [
    "# Get the validation accuracy of the best model\n",
    "# Get the accuracy corresponding to the lowest loss\n",
    "lowest_loss = validation_history['loss'].min()\n",
    "best_accuracy = validation_history.loc[validation_history['loss'] == lowest_loss, 'accuracy'].item()\n",
    "print(f'Best validation accuracy during training: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u_u2Zy5n92Q"
   },
   "source": [
    "# Test Data Prediction: VGG16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3Q29RWD7l_j"
   },
   "outputs": [],
   "source": [
    "# Generate test predictions and output the predictions to CSV named 'caltech256_custom_nn_prediction.csv'\n",
    "generate_test_predictions(\n",
    "    model =# CODE HERE\n",
    "    data_transforms =# CODE HERE\n",
    "    test_data_dir =# CODE HERE\n",
    "    output_csv_path =# CODE HERE\n",
    "    classes =# CODE HERE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
