{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdPBom61n6Dl"
   },
   "source": [
    "# Download Dataset\n",
    "- Download dataset from Box\n",
    "- Decompress dataset\n",
    "- The wafer map dataset is the same as the last homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaTTgEgBnw6G"
   },
   "outputs": [],
   "source": [
    "!wget -nc -O datasets.zip https://ucsb.box.com/shared/static/vz9pmd1h7eexf2qxr5odvjy0xew5aqpo.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XqNuwYYoJql"
   },
   "outputs": [],
   "source": [
    "!unzip -n datasets.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI4S6xMupDUs"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwAjofYzo5pa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Final, List, Tuple, NamedTuple, Type\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import cv2\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TODO: Type = NotImplemented\n",
    "\n",
    "print(\"PyTorch Version: \", torch.__version__)\n",
    "print(\"Torchvision Version: \", torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN3MkZVhpOqy"
   },
   "source": [
    "# Type Definition\n",
    "Define some types for clearer labeling of our interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYDAX8YMpO06"
   },
   "outputs": [],
   "source": [
    "TODO = NotImplemented  # NOTE: This is a placeholder for code that needs to be written\n",
    "\n",
    "Loss: Type = float\n",
    "Accuracy: Type = float\n",
    "\n",
    "\n",
    "class LossAcc(NamedTuple):\n",
    "    loss: Loss\n",
    "    accuracy: Accuracy\n",
    "\n",
    "\n",
    "class TrainValLossAcc(NamedTuple):\n",
    "    train: List[List[LossAcc]]\n",
    "    val: List[LossAcc]\n",
    "\n",
    "\n",
    "example_result_pair: LossAcc = LossAcc(1.6, 0.7)\n",
    "print(\"loss:\", example_result_pair.loss)\n",
    "print(\"accuracy:\", example_result_pair.accuracy)\n",
    "print(example_result_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tPIyp-bpRAz"
   },
   "source": [
    "Load training data from the training pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CXQfglipQYL"
   },
   "outputs": [],
   "source": [
    "# load training data from pickle file\n",
    "# assign the training data pickle file path\n",
    "train_pickle = # CODE HERE\n",
    "df_train: pd.DataFrame = pd.DataFrame.from_records(\n",
    "    np.load(train_pickle, allow_pickle=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNLnm1SepbcK"
   },
   "source": [
    "# Configurations\n",
    "Some configuration options that are required to make the dataset work with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0VL-TzbpbRc"
   },
   "outputs": [],
   "source": [
    "classes: Final[List[str]] = df_train.failureType.unique()\n",
    "\n",
    "num_classes: Final[int] = len(classes)\n",
    "\n",
    "num_epochs: Final[int] = 20\n",
    "\n",
    "batch_size: Final[int] = 16\n",
    "\n",
    "valid_set_size: Final[float] = 0.30\n",
    "\n",
    "NO_DIE: Final[int] = 0\n",
    "PASS_DIE: Final[int] = 1\n",
    "FAIL_DIE: Final[int] = 2\n",
    "NUM_DIE_STATES: Final[int] = 3\n",
    "\n",
    "\n",
    "print(\"Number of classes: \", num_classes)\n",
    "print(\"Number of epochs: \", num_epochs)\n",
    "print(\"Batch size: \", batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY3rskMyp8lN"
   },
   "source": [
    "Functions for\n",
    "- converting the failure type from string to int\n",
    "- resizing the wafer map and converting it into a 3-channel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k57fdCb0p0DS"
   },
   "outputs": [],
   "source": [
    "def convert_failure_type(failure_type: str) -> int:\n",
    "    # Create dictionary for converting string to numeric number\n",
    "    string2int: Final[Dict[str, int]] = {\n",
    "        \"Edge-Loc\": 0,\n",
    "        \"Scratch\": 1,\n",
    "        \"Donut\": 2,\n",
    "        \"Center\": 3,\n",
    "        \"Near-full\": 4,\n",
    "    }\n",
    "\n",
    "    return string2int[failure_type]\n",
    "\n",
    "\n",
    "def resize_wafer_map(\n",
    "    wafer_map: np.ndarray, output_shape: tuple = (64, 64)\n",
    ") -> np.ndarray:\n",
    "    # Final shape should be (3, 64, 64) for VGG16.\n",
    "    # To get the color dimension, we have many options:\n",
    "    #   + one-hot encode the integer labels\n",
    "    #   + use the MatPlotLib color map\n",
    "    #   + encode meaningful feature information\n",
    "    # Feel free to explore other options; you do not need to keep this given code!\n",
    "    resized = cv2.resize(wafer_map, output_shape, interpolation=cv2.INTER_NEAREST)\n",
    "    ret = np.eye(NUM_DIE_STATES)[resized]\n",
    "    ret = ret.transpose(2, 1, 0)  # (3, 64, 64)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLLtpHQkp-Ll"
   },
   "source": [
    "# Wafer Dataset Class\n",
    "The easiest way to work with PyTorch is to use its `Dataset` utility class. This class is used to load and wrap datapoints in PyTorch `tensor` objects, which are used to represent data in a device-specific way in PyTorch. `tensor`s allow for neural network training on GPUs.\n",
    "\n",
    "We provide the partial implementation of the wafer dataset below. You can see that it inherits from the `Dataset` class, and implements the `__len__` and `__getitem__` methods. The `__len__` method returns the number of datapoints in the dataset, and the `__getitem__` method returns a datapoint at a given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgG-yUJgp9Zt"
   },
   "outputs": [],
   "source": [
    "class WaferDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 size: Tuple[int,int]=(64,64),\n",
    "                 no_label: bool = False\n",
    "                 ):\n",
    "        self.df = df\n",
    "        self.size = size\n",
    "        self.no_label = no_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = self.df.loc[idx, \"waferMap\"]\n",
    "        # Use your resize_wafer_map function to resize the wafer map\n",
    "        img = # CODE HERE\n",
    "        img = img.astype(\"float32\")\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if(self.no_label):\n",
    "          label = torch.nan\n",
    "        else:\n",
    "          label = self.df.loc[idx, \"failureType\"]\n",
    "          # Use your convert_failure_type function to convert the failure type\n",
    "          label = # CODE HERE\n",
    "          label = torch.tensor(label)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCagEQPkzw3S"
   },
   "outputs": [],
   "source": [
    "all_dataset = WaferDataset(df_train, size=(64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LORXsdND-yTX"
   },
   "source": [
    "PyTorch provides the `torch.utils.data.random_split()` function to split a dataset. This method takes a dataset and a list of lengths/ratios, and returns a list of datasets of the specified lengths/ratios. If lengths are given, the sum of the lengths must equal the length of the original dataset. If ratios are given, the sum of the ratios must equal to 1. In the configuration block above, we have already computed `valid_set_size` for you, so you can use it to create a train/valid split.\n",
    "\n",
    "To get a consistent split, you are able to set your own random seed, though the process is more complex than for `scikit-learn`. To do so, add the argument `generator=torch.Generator().manual_seed(9)` to the `random_split()` function call. This split has produced a good class balance in our testing, but you are welcome to experiment with different `valid_set_size` and `manual_seed` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G91VDMMTqgsv"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = # CODE HERE\n",

""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mBjZ7OM3fES"
   },
   "source": [
    "Once arranged into a `Dataset`, we wrap the `Dataset` in a `Dataloader` object. This object is used to iterate over the dataset in batches, shuffle the dataset, and perform other useful operations, like parallel data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b3sSIzW3ghb"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EIeDOmqfKn"
   },
   "source": [
    "The below visualization of class label balances is provided for your reference. The train/valid split should produce similar class label balances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOgFemrWqAHd"
   },
   "outputs": [],
   "source": [
    "train_labels = pd.DataFrame(\n",
    "    Counter(l.item() for _, l in train_dataset).items(),\n",
    "    columns=[\"failureType\", \"count\"],\n",
    ")\n",
    "val_labels = pd.DataFrame(\n",
    "    Counter(l.item() for _, l in val_dataset).items(), columns=[\"failureType\", \"count\"]\n",
    ")\n",
    "train_labels[\"train\"] = True\n",
    "train_labels[\"ratio\"] = train_labels[\"count\"] / (train_labels[\"count\"].sum())\n",
    "val_labels[\"train\"] = False\n",
    "val_labels[\"ratio\"] = val_labels[\"count\"] / (val_labels[\"count\"].sum())\n",
    "labels = pd.concat([train_labels, val_labels])\n",
    "\n",
    "plt.bar(\n",
    "    labels[labels[\"train\"]][\"failureType\"],\n",
    "    labels[labels[\"train\"]][\"ratio\"],\n",
    "    label=\"train\",\n",
    "    alpha=0.7,\n",
    "    width=0.25,\n",
    "    align=\"edge\",\n",
    ")\n",
    "plt.bar(\n",
    "    labels[~labels[\"train\"]][\"failureType\"],\n",
    "    labels[~labels[\"train\"]][\"ratio\"],\n",
    "    label=\"val\",\n",
    "    alpha=0.7,\n",
    "    width=0.25,\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"failureType\")\n",
    "plt.ylabel(\"Proportion of samples in set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krjk8Nstyfx8"
   },
   "source": [
    "# Custom Neural Network\n",
    "Now we'll define a new network. We'll use this network to train on the wafer dataset. We'll use the `torch.nn` module to define the network. This module provides a number of useful building blocks for neural networks, including linear layers, convolution layers, dropout layers, maxpooling layers, activation functions, flatten function, and loss functions.\n",
    "\n",
    "We want a model with the following layers:\n",
    "+ Convolutional Layer 1: Input has 3 channels, output has 32 channels, kernel size is 3x3, and padding is 1.\n",
    "+ ReLU activation is applied.\n",
    "+ Dropout is applied with a probability of 0.5.\n",
    "+ Max-Pooling Layer 1: Performs max-pooling with a kernel size of 2x2 and a stride of 2.\n",
    "+ Convolutional Layer 2: Input has 32 channels, output has 64 channels, kernel size is 3x3, and padding is 1.\n",
    "+ ReLU activation is applied.\n",
    "+ Dropout is applied with a probability of 0.5.\n",
    "+ Max-Pooling Layer 2: Performs max-pooling with a kernel size of 2x2 and a stride of 2.\n",
    "+ Flatten layer is applied.\n",
    "+ Fully Connected Layer 1: Input size is 64\\*16\\*16, output size is 128.\n",
    "+ ReLU activation is applied.\n",
    "+ Dropout is applied with a probability of 0.5.\n",
    "+ Fully Connected Layer 2: Input size is 128, output size is 5 (for 5 output classes).\n",
    "\n",
    "\n",
    "If this is your absolute first interaction with PyTorch, you may find [this basic tutorial link](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) and [this CNN tutorial link](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network) to be of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAfDaho6yZK5"
   },
   "outputs": [],
   "source": [
    "class WaferNetwork(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(WaferNetwork, self).__init__()\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)\n",
    "\n",
    "        # Define dropout layers with the given dropout probability\n",
    "        self.dropout_conv = nn.Dropout2d(p=dropout_prob)\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.dropout_conv(x)  # Apply dropout to the output of the first convolutional layer\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.dropout_conv(x)  # Apply dropout to the output of the second convolutional layer\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten the tensor before passing it through fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)  # Apply dropout to the output of the first fully connected layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the WaferNetwork model with dropout\n",
    "model = WaferNetwork(dropout_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBIEFjmxsRvh"
   },
   "source": [
    "# Validate Function\n",
    "\n",
    "We will define the `validate` function that evaluates the performance of the model after every training epoch. This function simply puts the network into `eval` mode, then iterates over the dataset and computes the accuracy and loss of the network.\n",
    "\n",
    "Accuracy computations you can perform in similar manner as you have in the past assignments. For loss computations, we will use the `torch.nn.functional.cross_entropy` function, which assumes the network outputs class logits, and computes the cross entropy loss between the logits and the ground truth labels.\n",
    "\n",
    "Some components of models behave differently whether they're under training or validation. For example, dropout layers will randomly drop out nodes during training, but will not do so during validation. To handle this, PyTorch provides the `.train()` method to set the model into training mode and the `.eval()` method to set the model into validation mode. We will use this to set the model into validation mode before computing the accuracy and loss.\n",
    "\n",
    "Running data through a model is as simple as treating the model as a function! That is, `model(data)` will run the data through the model and return the output. We can then use this output to compute the accuracy and loss.\n",
    "\n",
    "The result of `torch.tensor` computations is always more `torch.tensor`s, so use the `.item()` method to get the actual value of the loss and/or accuracy when you are ready to store it.\n",
    "\n",
    "To leverage a GPU for speeding up the computation, you should move the model, data, and labels to the GPU when a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFAChrXRsReg"
   },
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, val_data: DataLoader) -> LossAcc:\n",
    "    \"\"\"\n",
    "    Returns the loss and accuracy of the model on the validation data.\n",
    "    \"\"\"\n",
    "    # Check for GPU availability:\n",
    "    # This code assigns the GPU (\"cuda\") as the device if a GPU is available;\n",
    "    # otherwise, it defaults to the CPU.\n",
    "    # Hint: search for `torch.device` and `torch.cuda.is_available`\n",
    "    # Hint: You should be able to code this in one line.\n",
    "    device = # CODE HERE\n",
    "\n",
    "    # Move the model to the GPU using the `.to` method\n",
    "    # Hint: This is a single method call.\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    # Hint: This is a single method call.\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # The below line sets up an environment that prevents\n",
    "    # the model from updating its weights during validation.\n",
    "    # This is helpful because we don't want to update the weights,\n",
    "    # so using this line saves us expensive gradient computations.\n",
    "    with torch.no_grad():\n",
    "        # Set up any tracking variables you'd like to use in order to compute\n",
    "        # the accuracy and average loss over all batches\n",
    "        # CODE HERE\n",
    "\n",
    "        # Iterate through the batches of validation data\n",
    "        # CODE HERE\n",
    "        for batch in val_data:# Provided\n",
    "\n",
    "            # Get the data and labels from the batch\n",
    "            # CODE HERE\n",
    "\n",
    "            # Move the data and labels to the GPU using the `.to` method\n",
    "            # CODE HERE\n",
    "\n",
    "            # Get the model's predictions for the data\n",
    "            # CODE HERE\n",
    "\n",
    "            # Calculate the average loss of this batch\n",
    "            # using `nn.functional.cross_entropy`\n",
    "            # CODE HERE\n",
    "\n",
    "            # Calculate the number of correct predictions and\n",
    "            # retrieve the number of samples in this batch\n",
    "            # CODE HERE\n",
    "\n",
    "        # Compute the accuracy and average loss over all batches.\n",
    "        accuracy = # CODE HERE\n",
    "        average_loss = # CODE HERE\n",
    "\n",
    "        return LossAcc(loss=average_loss, accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22Hr1XAyu9Pq"
   },
   "source": [
    "# Training Functions\n",
    "\n",
    "Now, to train a model, we have to write training functions. To ease understanding, we've broken the writing of a \"training loop\" into three functions: `train_batch` to handle a single batch, `train_epoch` to handle a single epoch, and `train_loop` to handle training as many steps as you would like.\n",
    "\n",
    "Remember, we're still using `nn.functional.cross_entropy` to compute the loss. Don't forget to set the model to `.train()` mode before training!\n",
    "\n",
    "After setting the model to training, we need to zero out the gradients from the previous step. This is because PyTorch accumulates gradients from each step, and we don't want to accumulate multiple steps of gradients after we've already used their gradients to update the model. We can zero out the gradients by calling the `.zero_grad()` method on the optimizer. Once we've computed the loss, we can call the `.backward()` method on the loss to compute the gradients. Finally, we can call the `.step()` method on the optimizer to update the model weights.\n",
    "\n",
    "To leverage a GPU for speeding up the computation, you should move the model, data, and labels to the GPU when a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8Lzk56Nr5h7"
   },
   "outputs": [],
   "source": [
    "def train_batch(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    inputs: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    ") -> LossAcc:\n",
    "    \"\"\"\n",
    "    Trains the model on a single batch of data.\n",
    "    Returns the loss and accuracy of the model on the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model to training mode\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Zero the gradients\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Get the model's predictions for the data\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Calculate the loss using `nn.functional.cross_entropy`\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Backpropagate the loss using `backward` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Update the model's weights using `step` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Calculate the accuracy\n",
    "        # CODE HERE\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        return LossAcc(loss.item(), accuracy)\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module, optimizer: optim.Optimizer, train_data: DataLoader\n",
    ") -> List[LossAcc]:\n",
    "    \"\"\"\n",
    "    Trains the model on a single epoch of data.\n",
    "    Returns a list of the loss and accuracy of the model on each batch.\n",
    "    \"\"\"\n",
    "    # Check for GPU availability:\n",
    "    # This code assigns the GPU (\"cuda\") as the device if a GPU is available;\n",
    "    # otherwise, it defaults to the CPU.\n",
    "    # Hint: search for `torch.device` and `torch.cuda.is_available`\n",
    "    # Hint: You should be able to code this in one line.\n",
    "    device = # CODE HERE\n",
    "\n",
    "    # Move the model to the GPU using the `.to` method\n",
    "    # CODE HERE (One line)\n",
    "\n",
    "    # Set up a list to store the LossAcc (loss and accuracy) of the batches\n",
    "    # CODE HERE\n",
    "\n",
    "    # Iterate through the batches of training data\n",
    "    # CODE HERE\n",
    "    for batch in train_data: # Provided\n",
    "\n",
    "        # Get the data and labels from the batch\n",
    "        # CODE HERE\n",
    "\n",
    "        # Move the data and labels to the GPU using the `.to` method\n",
    "        # CODE HERE\n",
    "\n",
    "        # Train the model on the batch with `train_batch`\n",
    "        # CODE HERE\n",
    "\n",
    "        # Add the LossAcc to the list\n",
    "        # CODE HERE\n",
    "\n",
    "    # Return the list of LossAcc\n",
    "    # CODE HERE\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    train_data: DataLoader,\n",
    "    val_data: DataLoader,\n",
    "    num_epochs: int = 10,\n",
    ") -> TrainValLossAcc:\n",
    "    \"\"\"\n",
    "    Trains the model on the training data for `num_epochs` epochs.\n",
    "    Returns the loss and accuracy of the model on the training and validation data,\n",
    "    taken per-batch in training and per-epoch in validation.\n",
    "    \"\"\"\n",
    "    # Set up a list to store the lists of training List[LossAcc]\n",
    "    # and another list to store the validation LossAcc\n",
    "    # CODE HERE\n",
    "\n",
    "    # Initialize variables to track the best model's state dictionary and\n",
    "    # best validation loss\n",
    "    # CODE HERE\n",
    "    best_model_state = model.state_dict()# Provided\n",
    "    best_valid_loss = float('inf')# Provided\n",
    "\n",
    "    # Iterate through the epochs\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training progress...'):\n",
    "\n",
    "        # Train the model on the epoch with `train_epoch`\n",
    "        # CODE HERE\n",
    "\n",
    "        # Validate the model on the validation data\n",
    "        # CODE HERE\n",
    "\n",
    "        # Add the loss and accuracy to the lists\n",
    "        # CODE HERE\n",
    "\n",
    "        # Update best model's state dictionary and best validation loss\n",
    "        # if validation loss is lower than the best validation loss\n",
    "        # CODE HERE\n",
    "        if(val_loss_acc.loss < best_valid_loss):# Provided\n",
    "            best_valid_loss = val_loss_acc.loss# Provided\n",
    "            best_model_state = model.state_dict()# Provided\n",
    "\n",
    "    # Load the model state dictionary with the best model's state dictionary\n",
    "    # CODE HERE\n",
    "    model.load_state_dict(best_model_state)# Provided\n",
    "\n",
    "    # Return the lists of loss and accuracy\n",
    "    return TrainValLossAcc(\n",
    "        train= # CODE HERE\n",
    "        val= # CODE HERE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3s6Ws-OzmVu"
   },
   "source": [
    "# Train Custom Neural Network\n",
    "\n",
    "Finally, we can train your first neural network! We'll use basic Stochastic Gradient Descent (SGD) as the optimization routine to train the network. We'll use a learning rate of 0.1, and train for 20 epochs, to give the model a good chance of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hT6Fr0a3zRAK"
   },
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "history: TrainValLossAcc = # CODE HERE\n",

""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_VHTU6J2mXb"
   },
   "source": [
    "We provide this visualization code, to give you a picture of what your model learned while it was training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-CtC6u5z5do"
   },
   "outputs": [],
   "source": [
    "training_history = pd.DataFrame.from_records(\n",
    "    [loss for batch_losses in history.train for loss in batch_losses],\n",
    "    columns=LossAcc._fields,\n",
    ")\n",
    "training_history[\"epoch\"] = training_history.index / len(train_dataloader)\n",
    "validation_history = pd.DataFrame.from_records(history.val, columns=LossAcc._fields)\n",
    "validation_history[\"epoch\"] = validation_history.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLrEIoRq2pvY"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(training_history[\"epoch\"], training_history[\"loss\"], label=\"Training\")\n",
    "ax[0].plot(validation_history[\"epoch\"], validation_history[\"loss\"], label=\"Validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(training_history[\"epoch\"], training_history[\"accuracy\"], label=\"Training\")\n",
    "ax[1].plot(\n",
    "    validation_history[\"epoch\"], validation_history[\"accuracy\"], label=\"Validation\"\n",
    ")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BAeoP9e46PH"
   },
   "outputs": [],
   "source": [
    "# Get the validation accuracy of the best model\n",
    "# Get the accuracy corresponding to the lowest loss\n",
    "lowest_loss = validation_history['loss'].min()\n",
    "best_accuracy = validation_history.loc[validation_history['loss'] == lowest_loss, 'accuracy'].item()\n",
    "print(f'Best validation accuracy during training: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1wmY6fo9TSE"
   },
   "source": [
    "# Test Data Prediction: Custom Neural Network Model\n",
    "Make Predictions for the Test Data. This function is already defined for you, you just need to call it and pass in the required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XRngcuf9X53"
   },
   "outputs": [],
   "source": [
    "def generate_test_predictions(model: WaferNetwork,\n",
    "                              test_pickle: str,\n",
    "                              output_csv_path: str,\n",
    "                              classes: List[str],\n",
    "                              ):\n",
    "    # load test dataframe from pickle\n",
    "    df_test: pd.DataFrame = pd.DataFrame.from_records(\n",
    "        np.load(test_pickle, allow_pickle=True)\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_dataset = WaferDataset(df_test, size=(64,64), no_label=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize a list to store the predicted class labels\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate through the test dataset and make predictions\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Convert integer labels to class names\n",
    "            predicted_class_names = [classes[p] for p in predicted]\n",
    "\n",
    "            # predicted class labels\n",
    "            predicted_labels.extend(predicted_class_names)\n",
    "\n",
    "    # Create a DataFrame to store basenames and predicted labels\n",
    "    data = {'Predictions': predicted_labels}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the sorted predictions to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Predictions saved to {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yj_5LY45-4ew"
   },
   "outputs": [],
   "source": [
    "# Generate test predictions and output the predictions to CSV named 'wafermap_custom_nn_prediction.csv'\n",
    "generate_test_predictions(\n",
    "    model = # CODE HERE\n",
    "    test_pickle = # CODE HERE\n",
    "    output_csv_path = # CODE HERE\n",
    "    classes = # CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdBQp71t39Ty"
   },
   "source": [
    "# Fine-Tune VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQTLE4N44VQx"
   },
   "source": [
    "Now, we will be fine-tuning a VGG16 model to perform the same wafer classification task and compare the performance against the previous model which is trained from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbxtR7kREi0f"
   },
   "source": [
    "Overview of Fine-Tuning VGG16\n",
    "\n",
    "VGG16 is a popular deep convolutional neural network architecture used for various computer vision tasks. It consists of two main components: the \"features\" part and the \"classifier\" part. Fine-tuning VGG16 involves adjusting these components to adapt the pre-trained model to a new task.\n",
    "\n",
    "Components of VGG16\n",
    "1. Features Layers:\n",
    "The \"features\" part of VGG16 comprises a stack of convolutional and max-pooling layers. These layers serve as feature extractors and are responsible for capturing hierarchical features from the input image.\n",
    "These layers are typically frozen during fine-tuning. Freezing means that the weights of these layers are not updated during training to preserve the pre-trained knowledge.\n",
    "2. Classifier Layers:\n",
    "The \"classifier\" part of VGG16 consists of fully connected layers (also known as dense layers). These layers are responsible for making predictions based on the extracted features.\n",
    "During fine-tuning, the classifier layers are often replaced by a new set of fully connected layers to adapt the model to the specific task.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnbVLEPTExn-"
   },
   "source": [
    "Load the Pre-Trained VGG16 Model:\n",
    "\n",
    "Begin by loading the pre-trained VGG16 model with weights 'IMAGENET1K_V1', which includes both the features and classifier components. You can do this using PyTorch's torchvision.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EEZ8p7k2qsC"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# load VGG16 models with weights 'IMAGENET1K_V1'\n",
    "# CODE HERE\n",
    "model = # CODE HERE\n",

""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBNEPlAcD88p"
   },
   "source": [
    "Freeze the Features Layers:\n",
    "\n",
    "Disable gradient updates to prevent them from changing during training. This step ensures that the model retains its ability to extract relevant features from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGU_IYWq6He2"
   },
   "outputs": [],
   "source": [
    "# Get the features part of the model\n",
    "features = model.features # Provided\n",
    "\n",
    "# Freeze the feature layers\n",
    "# CODE HERE\n",
    "for param in features.parameters(): # Provided\n",
    "    param.requires_grad = False # Provided END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmoBdEG5FXvr"
   },
   "source": [
    "Define and Replace the Classifier:\n",
    "\n",
    "+ Replace only the last layer to have 5 output units for the wafer dataset. Note: Last classifier layer has 4096 input units.\n",
    "+ Set the last layer to require gradients for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-1XLZEs5enP"
   },
   "outputs": [],
   "source": [
    "# Modify the last layer to have 5 output units for the wafer dataset\n",
    "# CODE HERE\n",
    "num_classes = 5# Provided\n",
    "model.classifier[6] = nn.Linear(4096, num_classes) # Provided\n",
    "\n",
    "# Set the last layer to require gradients for fine-tuning\n",
    "# CODE HERE\n",
    "model.classifier[6].requires_grad = True # Provided END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR3Vi4PG6RRJ"
   },
   "source": [
    "# Create Dataset (VGG16)\n",
    "VGG16 is pre-trained on (224, 224) image size so we need to recreate the dataset and data loader with wafermaps resized to that size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzOAsDSn6oJf"
   },
   "outputs": [],
   "source": [
    "# Redefine the `all_dataset` with size = (224, 224)\n",
    "all_dataset = # CODE HERE\n",
    "\n",
    "# Split the dataset into training and validation with the same configuration as above\n",
    "# CODE HERE\n",
    "train_dataset, val_dataset = # CODE HERE\n",
    "\n",
    "# Create the data loader with the same configuration as above\n",
    "# CODE HERE\n",
    "train_dataloader = # CODE HERE\n",
    "val_dataloader = # CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSXrVI-V4Voq"
   },
   "source": [
    "# Fine-Tune VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyfsOfAO4Q3m"
   },
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "history: TrainValLossAcc = # CODE HERE\n",

""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIJTaDkY4SjD"
   },
   "source": [
    "We provide this visualization code, to give you a picture of what your model learned while it was training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vEQM-6K4S_W"
   },
   "outputs": [],
   "source": [
    "training_history = pd.DataFrame.from_records(\n",
    "    [loss for batch_losses in history.train for loss in batch_losses],\n",
    "    columns=LossAcc._fields,\n",
    ")\n",
    "training_history[\"epoch\"] = training_history.index / len(train_dataloader)\n",
    "validation_history = pd.DataFrame.from_records(history.val, columns=LossAcc._fields)\n",
    "validation_history[\"epoch\"] = validation_history.index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20v5Zp0z4UbS"
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(training_history[\"epoch\"], training_history[\"loss\"], label=\"Training\")\n",
    "ax[0].plot(validation_history[\"epoch\"], validation_history[\"loss\"], label=\"Validation\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(training_history[\"epoch\"], training_history[\"accuracy\"], label=\"Training\")\n",
    "ax[1].plot(\n",
    "    validation_history[\"epoch\"], validation_history[\"accuracy\"], label=\"Validation\"\n",
    ")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3Q29RWD7l_j"
   },
   "outputs": [],
   "source": [
    "# Get the validation accuracy of the best model\n",
    "# Get the accuracy corresponding to the lowest loss\n",
    "lowest_loss = validation_history['loss'].min()\n",
    "best_accuracy = validation_history.loc[validation_history['loss'] == lowest_loss, 'accuracy'].item()\n",
    "print(f'Best validation accuracy during training: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM0Vxwxh_K-k"
   },
   "source": [
    "# Test Data Prediction: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yBmZQXr_LXC"
   },
   "outputs": [],
   "source": [
    "# Generate test predictions and output the predictions to CSV named 'wafermap_vgg16_nn_prediction.csv'\n",
    "generate_test_predictions(\n",
    "    model = # CODE HERE\n",
    "    test_pickle = # CODE HERE\n",
    "    output_csv_path = # CODE HERE\n",
    "    classes = # CODE HERE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
